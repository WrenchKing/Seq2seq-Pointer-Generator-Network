{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4578,
     "status": "ok",
     "timestamp": 1575204553604,
     "user": {
      "displayName": "Alexandros Dorodoulis",
      "photoUrl": "",
      "userId": "07816985308410186091"
     },
     "user_tz": -60
    },
    "id": "_e5UcD5lD0BJ",
    "outputId": "3935d856-a123-424a-c358-e8c1874dd155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cpu\n"
     ]
    }
   ],
   "source": [
    "#from data_generator_with_sos import generate\n",
    "# data generator in next cell. It's just to translate number words (e.g. 'eight','nine',etc) to digits\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device in use:\", device)\n",
    "\n",
    "NUM_INPUTS = 28 #No. of possible characters\n",
    "NUM_OUTPUTS = 12  # (0-9 + '#')\n",
    "\n",
    "### Hyperparameters and general configs\n",
    "MAX_SEQ_LEN = 8\n",
    "MIN_SEQ_LEN = 5\n",
    "TRAINING_SIZE = 100\n",
    "LEARNING_RATE = 0.003\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Hidden size of enc and dec need to be equal if last hidden of encoder becomes init hidden of decoder\n",
    "# Otherwise we would need e.g. a linear layer to map to a space with the correct dimension\n",
    "NUM_UNITS_ENC = NUM_UNITS_DEC = 256\n",
    "HIDDEN_DIM = 512\n",
    "TEST_SIZE = 100\n",
    "EPOCHS = 10\n",
    "TEACHER_FORCING = 0.5\n",
    "NUM_OF_BATCHES=8\n",
    "\n",
    "# assert TRAINING_SIZE % NUM_OF_BATCHES == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_text = {\n",
    "    \"1\": \"one\",\n",
    "    \"2\": \"two\",\n",
    "    \"3\": \"three\",\n",
    "    \"4\": \"four\",\n",
    "    \"5\": \"five\",\n",
    "    \"6\": \"six\",\n",
    "    \"7\": \"seven\",\n",
    "    \"8\": \"eight\",\n",
    "    \"9\": \"nine\",\n",
    "}\n",
    "EOS = \"#\"\n",
    "SOS='*'\n",
    "PAD = \"0\"\n",
    "\n",
    "input_characters = \" \".join(target_to_text.values())\n",
    "valid_characters = [\n",
    "    PAD,\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    EOS,\n",
    "    SOS,\n",
    "] + list(set(input_characters))\n",
    "\n",
    "\n",
    "def print_valid_characters():\n",
    "    l = \"\"\n",
    "    for i, c in enumerate(valid_characters):\n",
    "        l += \"'%s'=%i,\\t\" % (c, i)\n",
    "    print(\"Number of valid characters:\", len(valid_characters))\n",
    "    print(l)\n",
    "\n",
    "\n",
    "ninput_chars = len(valid_characters)\n",
    "\n",
    "\n",
    "def generate(\n",
    "    num_batches=10, batch_size=100, min_len=3, max_len=3, invalid_set=set()\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates random sequences of integers and translates them to text i.e. 1->'one'.\n",
    "\n",
    "    :param batch_size: number of samples to return\n",
    "    :param min_len: minimum length of target\n",
    "    :param max_len: maximum length of target\n",
    "    :param invalid_set: set of invalid 'text targets out', e.g. if we want to avoid samples\n",
    "    that are already in the training set\n",
    "    \"\"\"\n",
    "\n",
    "    text_inputs = []\n",
    "    int_inputs = []\n",
    "    text_targets_in = []\n",
    "    text_targets_out = []\n",
    "    int_targets_in = []\n",
    "    int_targets_out = []\n",
    "    inputs = []\n",
    "    targets_in = []\n",
    "    targets_out = []\n",
    "    batch_target_max_len = np.zeros((1, num_batches))\n",
    "    targets_mask = []\n",
    "    batch_input_max_len = np.zeros((1, num_batches))\n",
    "    inputs_len = []\n",
    "    _printed_warning = False\n",
    "    # loop through number of batches\n",
    "    for i in range(num_batches):\n",
    "        temp_text_inputs = []\n",
    "        temp_int_inputs = []\n",
    "        temp_text_targets_in = []\n",
    "        temp_text_targets_out = []\n",
    "        temp_int_targets_in = []\n",
    "        temp_int_targets_out = []\n",
    "        iterations = 0\n",
    "        # while loop until number of rows per batch is reached\n",
    "        while len(temp_text_inputs) < batch_size:\n",
    "            iterations += 1\n",
    "\n",
    "            # choose random sequence length\n",
    "            tar_len = np.random.randint(min_len, max_len + 1)\n",
    "\n",
    "            # list of text digits\n",
    "            text_target = inp_str = \"\".join(\n",
    "                map(str, np.random.randint(1, 10, tar_len))\n",
    "            )\n",
    "            text_target_in = SOS + text_target\n",
    "            text_target_out = text_target + EOS\n",
    "\n",
    "            # generate the targets as a list of integers\n",
    "            int_target_in = map(\n",
    "                lambda c: valid_characters.index(c), text_target_in\n",
    "            )\n",
    "            int_target_in = list(int_target_in)\n",
    "\n",
    "            int_target_out = map(\n",
    "                lambda c: valid_characters.index(c), text_target_out\n",
    "            )\n",
    "            int_target_out = list(int_target_out)\n",
    "\n",
    "            # generate the text input\n",
    "            text_input = \" \".join(map(lambda k: target_to_text[k], inp_str))\n",
    "\n",
    "            # generate the inputs as a list of integers\n",
    "            int_input = map(lambda c: valid_characters.index(c), text_input)\n",
    "            int_input = list(int_input)\n",
    "\n",
    "            if not _printed_warning and iterations > 5 * batch_size:\n",
    "                print(\n",
    "                    \"WARNING: doing a lot of iterations because I'm trying to generate a batch that does not\"\n",
    "                    \" contain samples from 'invalid_set'.\"\n",
    "                )\n",
    "                _printed_warning = True\n",
    "\n",
    "            if text_target_out in invalid_set:\n",
    "                continue\n",
    "            # append created row to temp arrays\n",
    "            temp_text_inputs.append(text_input)\n",
    "            temp_int_inputs.append(int_input)\n",
    "            temp_text_targets_in.append(text_target_in)\n",
    "            temp_text_targets_out.append(text_target_out)\n",
    "            temp_int_targets_in.append(int_target_in)\n",
    "            temp_int_targets_out.append(int_target_out)\n",
    "        # turn the temp arrays into tensors and pad them to max length\n",
    "\n",
    "        # append completed temp batch array to full array of batches\n",
    "        text_inputs.append(temp_text_inputs)\n",
    "        int_inputs.append(temp_int_inputs)\n",
    "        text_targets_in.append(temp_text_targets_in)\n",
    "        text_targets_out.append(temp_text_targets_out)\n",
    "        int_targets_in.append(temp_int_targets_in)\n",
    "        int_targets_out.append(temp_int_targets_out)\n",
    "\n",
    "        max_target_out_len = max(map(len, int_targets_out[-1]))\n",
    "        max_input_len = max(map(len, int_inputs[-1]))\n",
    "        targets_mask_tmp = np.zeros((batch_size, max_target_out_len))\n",
    "        add_targets_out = np.full((batch_size, max_target_out_len), int(PAD))\n",
    "        len_arr = [-len(thing) for thing in temp_int_inputs]\n",
    "        sorted_arr = np.argsort(len_arr)\n",
    "        add_targets_in = np.full((batch_size, max_target_out_len), int(PAD))\n",
    "        add_inputs = np.full((batch_size, max_input_len), int(PAD))\n",
    "        tmp_inputs_len = np.zeros(len(sorted_arr))\n",
    "        for short_index, row in enumerate(sorted_arr):\n",
    "            tmp_element = temp_int_inputs[row]\n",
    "            add_inputs[short_index, : len(tmp_element)] = tmp_element\n",
    "            tmp_inputs_len[short_index] = len(tmp_element)\n",
    "\n",
    "            tmp_element = temp_int_targets_in[row]\n",
    "            add_targets_in[short_index, : len(tmp_element)] = tmp_element\n",
    "\n",
    "            tmp_element = temp_int_targets_out[row]\n",
    "            add_targets_out[short_index, : len(tmp_element)] = tmp_element\n",
    "            targets_mask_tmp[short_index, : len(tmp_element)] = 1\n",
    "        inputs_len.append(tmp_inputs_len)\n",
    "        targets_mask.append(targets_mask_tmp)\n",
    "        inputs.append(add_inputs.astype(\"int32\"))\n",
    "        targets_in.append(add_targets_in.astype(\"int32\"))\n",
    "        targets_out.append(add_targets_out.astype(\"int32\"))\n",
    "        target_in_seq_lengths = torch.LongTensor(\n",
    "            list(map(len, temp_int_targets_in))\n",
    "        )\n",
    "        input_seq_lengths = torch.LongTensor(list(map(len, temp_int_inputs)))\n",
    "\n",
    "        batch_target_max_len[0, i] = target_in_seq_lengths.max()\n",
    "        batch_input_max_len[0, i] = input_seq_lengths.max()\n",
    "    return (\n",
    "        inputs,\n",
    "        batch_input_max_len.astype(\"int32\"),\n",
    "        targets_in,\n",
    "        targets_out,\n",
    "        batch_target_max_len.astype(\"int32\"),\n",
    "        targets_mask,\n",
    "        text_inputs,\n",
    "        text_targets_in,\n",
    "        text_targets_out,\n",
    "        inputs_len,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    batch_size = 3\n",
    "    (\n",
    "        inputs,\n",
    "        inputs_seqlen,\n",
    "        targets_in,\n",
    "        targets_out,\n",
    "        targets_seqlen,\n",
    "        targets_mask,\n",
    "        text_inputs,\n",
    "        text_targets_in,\n",
    "        text_targets_out,\n",
    "        inputs_len,\n",
    "    ) = generate(8, 10, min_len=1, max_len=2)\n",
    "\n",
    "    print_valid_characters()\n",
    "    print(\"Stop/start character = #\")\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        print(\"\\nSAMPLE\", i)\n",
    "        print(\"TEXT INPUTS:\\t\\t\\t\", text_inputs[i])\n",
    "        print(\"ENCODED INPUTS:\\t\\t\\t\\n\", inputs[i])\n",
    "        print(\"INPUTS SEQUENCE LENGTH:\\t\\n\", inputs_seqlen)\n",
    "        print(\"TEXT TARGETS INPUT:\\t\\t\", text_targets_in[i])\n",
    "        print(\"TEXT TARGETS OUTPUT:\\t\", text_targets_out[i])\n",
    "        print(\"ENCODED TARGETS INPUT:\\t\\n\", targets_in[i])\n",
    "        print(\"ENCODED TARGETS OUTPUT:\\t\\n\", targets_out[i])\n",
    "        print(\"TARGETS SEQUENCE LENGTH:\", targets_seqlen)\n",
    "        print(\"TARGETS MASK:\\t\\t\\t\\n\", targets_mask[i])\n",
    "        print(\"INPUTS LEN:\\t\\t\\t\\n\", inputs_len[i])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNWiuJAvD0Bg"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, self.emb_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.emb_size,\n",
    "            self.hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, hidden, inputs_len):\n",
    "        # Input shape [batch, seq_in_len]\n",
    "        # inputs = [inputs[0],inputs[2]]\n",
    "        inputs = inputs.long()\n",
    "\n",
    "        # Embedded shape [batch, seq_in_len, embed]\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        # embedded = embedded.view(embedded.shape[0]*embedded.shape[1],embedded.shape[2],embedded.shape[3])\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, inputs_len, batch_first=True\n",
    "        )\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        # Output shape [batch, seq_in_len, embed]\n",
    "        # Hidden shape [1, batch, embed], last hidden state of the GRU cell\n",
    "       \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outputs, batch_first=True\n",
    "        )\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2], hidden[-1]), dim=1)))\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        init = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        return init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9bkqDKVfRHw"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear((hidden_size * 2) + hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "\n",
    "        # hidden = [batch size, dec hid dim]\n",
    "        # encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        # mask = [batch size, src sent len]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # encoder output =  [33, 8, 512], hidden = [8, 256]\n",
    "        # print(encoder_outputs.shape[0], encoder_outputs.shape[1])\n",
    "\n",
    "        # repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        # hidden = [batch size, src sent len, dec hid dim]\n",
    "        # encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "\n",
    "        energy = torch.tanh(\n",
    "            self.attn(torch.cat((hidden, encoder_outputs), dim=2))\n",
    "        )\n",
    "\n",
    "        # energy = [batch size, src sent len, dec hid dim]\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        # energy = [batch size, dec hid dim, src sent len]\n",
    "\n",
    "        # v = [dec hid dim]\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        # v = [batch size, 1, dec hid dim]\n",
    "\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        # attention = [batch size, src sent len]\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtQnQrtHD0Bm"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, emb_size, output_size, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.emb_size = emb_size\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.emb_size)\n",
    "        self.out = nn.Linear(\n",
    "            (self.hidden_size * 2) + self.hidden_size + self.emb_size,\n",
    "            output_size\n",
    "        )\n",
    "        self.rnn = nn.GRU((self.hidden_size * 2) + self.emb_size, self.hidden_size,num_layers=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, hidden, encoder_outputs, mask):\n",
    "        # Input shape: [batch, output_len]\n",
    "        # Hidden shape: [seq_len=1, batch_size, hidden_dim] (the last hidden state of the encoder)\n",
    "        dec_input = inputs.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(dec_input))\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "      \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "    \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        out, hidden = self.rnn(rnn_input,hidden.unsqueeze(0))\n",
    "        assert (out == hidden).all()\n",
    "        embedded = embedded.squeeze(0)\n",
    "        out = out.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((out, weighted, embedded), dim=1))\n",
    "      # [batch_size x seq_len x output_size]\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        return output, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ2vLb3CfRIE"
   },
   "outputs": [],
   "source": [
    "def create_mask(src):\n",
    "    mask = src != 0  # .permute(1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EyWCT1sLD0Bu"
   },
   "outputs": [],
   "source": [
    "def forward_pass(\n",
    "    encoder, decoder, x, t, t_in, x_len, criterion, teacher_forcing_ratio=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a forward pass through the whole model.\n",
    "\n",
    "    :param encoder:\n",
    "    :param decoder:\n",
    "    :param x: input to the encoder, shape [batch, seq_in_len]\n",
    "    :param t: target output predictions for decoder, shape [batch, seq_t_len]\n",
    "    :param criterion: loss function\n",
    "    :param max_t_len: maximum target length\n",
    "\n",
    "    :return: output (after log-softmax), loss, accuracy (per-symbol)\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    trg_len = t_in.shape[1]\n",
    "    trg_vocab_size = NUM_OUTPUTS\n",
    "\n",
    "    # tensor to store decoder outputs\n",
    "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "\n",
    "    # Run encoder and get last hidden state (and output)\n",
    "\n",
    "    enc_h = encoder.init_hidden(batch_size)\n",
    "    enc_out, enc_h = encoder(x, enc_h, x_len)\n",
    "\n",
    "    # first input to the decoder is the <sos> tokens\n",
    "    inputs = t_in[:, 0]\n",
    "    dec_h = enc_h\n",
    "    mask = create_mask(x)\n",
    "    for i in range(1, trg_len + 1):\n",
    "\n",
    "        # insert input token embedding, previous hidden state, all encoder hidden states\n",
    "        #  and mask\n",
    "        # receive output tensor (predictions) and new hidden state\n",
    "        output, dec_h, _ = decoder(inputs, dec_h, enc_out, mask)\n",
    "\n",
    "        # place predictions in a tensor holding predictions for each token\n",
    "        outputs[i - 1] = output\n",
    "\n",
    "        # decide if we are going to use teacher forcing or not\n",
    "        teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "        # get the highest predicted token from our predictions\n",
    "        top1 = output.argmax(1)\n",
    "        # if teacher forcing, use actual next token as next input\n",
    "        # if not, use predicted token\n",
    "        if i < trg_len:\n",
    "            inputs = t_in[:, i] if teacher_force else top1\n",
    "\n",
    "    out = outputs.permute(1, 2, 0)\n",
    "    # Shape: [batch_size x num_classes x out_sequence_len], with second dim containing log probabilities\n",
    "    loss = criterion(out, t)\n",
    "    pred = get_pred(log_probs=out)\n",
    "    accuracy = (pred == t).type(torch.FloatTensor).mean()\n",
    "\n",
    "    return out, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G_hLF0GD0By"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    inputs,\n",
    "    targets,\n",
    "    targets_in,\n",
    "    criterion,\n",
    "    enc_optimizer,\n",
    "    dec_optimizer,\n",
    "    epoch,\n",
    "    inputs_len\n",
    "):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (x, t, t_in, x_len) in enumerate(\n",
    "        zip(inputs, targets, targets_in, inputs_len)\n",
    "    ):\n",
    "        # print(x.shape)\n",
    "        x = torch.LongTensor(x).to(device)\n",
    "        t = torch.LongTensor(t).to(device)\n",
    "        t_in = torch.LongTensor(t_in).to(device)\n",
    "        x_len = torch.LongTensor(x_len).to(device)\n",
    "\n",
    "        enc_optimizer.zero_grad()\n",
    "        dec_optimizer.zero_grad()\n",
    "\n",
    "        # print(batch_idx)\n",
    "        #         inputs = inputs.to(device)\n",
    "        #         targets = targets.long()\n",
    "        #         targets_in = targets_in.long()\n",
    "        out, loss, accuracy = forward_pass(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            x,\n",
    "            t,\n",
    "            t_in,\n",
    "            x_len,\n",
    "            criterion,\n",
    "            teacher_forcing_ratio=TEACHER_FORCING\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(\n",
    "                \"Epoch {} [{}/{} ({:.0f}%)]\\tTraining loss: {:.4f} \\tTraining accuracy: {:.1f}%\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(x),\n",
    "                    TRAINING_SIZE * NUM_OF_BATCHES,\n",
    "                    100.0\n",
    "                    * batch_idx\n",
    "                    * len(x)\n",
    "                    / (TRAINING_SIZE * NUM_OF_BATCHES),\n",
    "                    loss.item(),\n",
    "                    100.0 * accuracy.item()\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWR1jGl5D0B3"
   },
   "outputs": [],
   "source": [
    "def test(encoder, decoder, inputs, targets, targets_in, inputs_len, criterion):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.view(inputs.shape[1], inputs.shape[2])\n",
    "        targets = targets.view(targets.shape[1], targets.shape[2])\n",
    "        targets_in = targets_in.view(targets_in.shape[1], targets_in.shape[2])\n",
    "        inputs_len = torch.LongTensor(inputs_len[0]).to(device)\n",
    "\n",
    "        out, loss, accuracy = forward_pass(encoder,decoder,inputs, targets,targets_in,\n",
    "                                inputs_len,criterion,teacher_forcing_ratio=TEACHER_FORCING\n",
    "        )\n",
    "        # print(out.shape,targets_in.shape)\n",
    "    return out, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fr_FfcipD0B-"
   },
   "outputs": [],
   "source": [
    "def numbers_to_text(seq):\n",
    "    return \"\".join([str(to_np(i)) if to_np(i) != 10 else \"#\" for i in seq])\n",
    "\n",
    "\n",
    "def to_np(x):\n",
    "    return x.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_pred(log_probs):\n",
    "    \"\"\"\n",
    "    Get class prediction (digit prediction) from the net's output (the log_probs)\n",
    "    :param log_probs: Tensor of shape [batch_size x n_classes x sequence_len]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return torch.argmax(log_probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 231769,
     "status": "ok",
     "timestamp": 1575204780896,
     "user": {
      "displayName": "Alexandros Dorodoulis",
      "photoUrl": "",
      "userId": "07816985308410186091"
     },
     "user_tz": -60
    },
    "id": "k1lh5zCfD0CB",
    "outputId": "db7527ce-7805-4706-8ee3-e4af3581e47a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/800 (0%)]\tTraining loss: 2.5533 \tTraining accuracy: 16.7%\n",
      "\n",
      "Test set: Average loss: 2.2217 \tAccuracy: 24.333%\n",
      "\n",
      "Examples: prediction | input\n",
      "68873#3#6 \t four seven nine five seven six\n",
      "1779#7##9 \t one six one eight four three three nine\n",
      "777337### \t five seven one nine eight seven\n",
      "9775#7#7# \t seven eight seven three one three five five\n",
      "771797##9 \t nine two seven eight five six two four\n",
      "566#6#66# \t five four one four five\n",
      "77675#6#6 \t five six two nine three eight two\n",
      "261616##6 \t three four nine three one\n",
      "499#9#9#9 \t six six six one one three\n",
      "\n",
      "Epoch 2 [0/800 (0%)]\tTraining loss: 2.4005 \tTraining accuracy: 25.0%\n",
      "\n",
      "Test set: Average loss: 1.8242 \tAccuracy: 32.111%\n",
      "\n",
      "Examples: prediction | input\n",
      "388888888 \t four seven nine five seven six\n",
      "199979### \t one six one eight four three three nine\n",
      "77175#### \t five seven one nine eight seven\n",
      "399779797 \t seven eight seven three one three five five\n",
      "877999999 \t nine two seven eight five six two four\n",
      "56963#### \t five four one four five\n",
      "7765791## \t five six two nine three eight two\n",
      "29199963# \t three four nine three one\n",
      "499999999 \t six six six one one three\n",
      "\n",
      "Epoch 3 [0/800 (0%)]\tTraining loss: 2.1101 \tTraining accuracy: 26.4%\n",
      "\n",
      "Test set: Average loss: 1.6729 \tAccuracy: 36.333%\n",
      "\n",
      "Examples: prediction | input\n",
      "36858#### \t four seven nine five seven six\n",
      "18997#### \t one six one eight four three three nine\n",
      "73353#### \t five seven one nine eight seven\n",
      "3999775## \t seven eight seven three one three five five\n",
      "855599### \t nine two seven eight five six two four\n",
      "5666##### \t five four one four five\n",
      "75556#### \t five six two nine three eight two\n",
      "299999999 \t three four nine three one\n",
      "444419### \t six six six one one three\n",
      "\n",
      "Epoch 4 [0/800 (0%)]\tTraining loss: 1.7028 \tTraining accuracy: 40.3%\n",
      "\n",
      "Test set: Average loss: 1.5433 \tAccuracy: 39.000%\n",
      "\n",
      "Examples: prediction | input\n",
      "385883### \t four seven nine five seven six\n",
      "18937#### \t one six one eight four three three nine\n",
      "78533#533 \t five seven one nine eight seven\n",
      "399999999 \t seven eight seven three one three five five\n",
      "839999999 \t nine two seven eight five six two four\n",
      "5693#5### \t five four one four five\n",
      "75361#53# \t five six two nine three eight two\n",
      "29391#939 \t three four nine three one\n",
      "469393#99 \t six six six one one three\n",
      "\n",
      "Epoch 5 [0/800 (0%)]\tTraining loss: 1.9216 \tTraining accuracy: 29.2%\n",
      "\n",
      "Test set: Average loss: 1.4649 \tAccuracy: 41.778%\n",
      "\n",
      "Examples: prediction | input\n",
      "36838#### \t four seven nine five seven six\n",
      "18999#### \t one six one eight four three three nine\n",
      "78755##5# \t five seven one nine eight seven\n",
      "399999999 \t seven eight seven three one three five five\n",
      "8799799#9 \t nine two seven eight five six two four\n",
      "5666#5#5# \t five four one four five\n",
      "715517### \t five six two nine three eight two\n",
      "211797##9 \t three four nine three one\n",
      "422499### \t six six six one one three\n",
      "\n",
      "Epoch 6 [0/800 (0%)]\tTraining loss: 1.5781 \tTraining accuracy: 43.1%\n",
      "\n",
      "Test set: Average loss: 1.3920 \tAccuracy: 44.778%\n",
      "\n",
      "Examples: prediction | input\n",
      "388888888 \t four seven nine five seven six\n",
      "18873#### \t one six one eight four three three nine\n",
      "78735#### \t five seven one nine eight seven\n",
      "9997##### \t seven eight seven three one three five five\n",
      "87975#### \t nine two seven eight five six two four\n",
      "56685#### \t five four one four five\n",
      "74561#### \t five six two nine three eight two\n",
      "21193#### \t three four nine three one\n",
      "43939#### \t six six six one one three\n",
      "\n",
      "Epoch 7 [0/800 (0%)]\tTraining loss: 1.8299 \tTraining accuracy: 43.1%\n",
      "\n",
      "Test set: Average loss: 1.1626 \tAccuracy: 50.556%\n",
      "\n",
      "Examples: prediction | input\n",
      "668873#8# \t four seven nine five seven six\n",
      "1839477## \t one six one eight four three three nine\n",
      "78333#### \t five seven one nine eight seven\n",
      "333379### \t seven eight seven three one three five five\n",
      "839979999 \t nine two seven eight five six two four\n",
      "5667633## \t five four one four five\n",
      "74665#### \t five six two nine three eight two\n",
      "281936#9# \t three four nine three one\n",
      "424499#99 \t six six six one one three\n",
      "\n",
      "Epoch 8 [0/800 (0%)]\tTraining loss: 1.1406 \tTraining accuracy: 50.0%\n",
      "\n",
      "Test set: Average loss: 1.2887 \tAccuracy: 50.111%\n",
      "\n",
      "Examples: prediction | input\n",
      "86358888# \t four seven nine five seven six\n",
      "1849777## \t one six one eight four three three nine\n",
      "77777778# \t five seven one nine eight seven\n",
      "339797979 \t seven eight seven three one three five five\n",
      "879955993 \t nine two seven eight five six two four\n",
      "56936#### \t five four one four five\n",
      "745573333 \t five six two nine three eight two\n",
      "2198733#3 \t three four nine three one\n",
      "439494999 \t six six six one one three\n",
      "\n",
      "Epoch 9 [0/800 (0%)]\tTraining loss: 1.2341 \tTraining accuracy: 51.4%\n",
      "\n",
      "Test set: Average loss: 1.1560 \tAccuracy: 52.778%\n",
      "\n",
      "Examples: prediction | input\n",
      "388387883 \t four seven nine five seven six\n",
      "188944#7# \t one six one eight four three three nine\n",
      "78731353# \t five seven one nine eight seven\n",
      "333377### \t seven eight seven three one three five five\n",
      "833999999 \t nine two seven eight five six two four\n",
      "5796##### \t five four one four five\n",
      "7117556## \t five six two nine three eight two\n",
      "211977### \t three four nine three one\n",
      "4344999## \t six six six one one three\n",
      "\n",
      "Epoch 10 [0/800 (0%)]\tTraining loss: 1.6189 \tTraining accuracy: 45.8%\n",
      "\n",
      "Test set: Average loss: 1.4020 \tAccuracy: 45.111%\n",
      "\n",
      "Examples: prediction | input\n",
      "383387877 \t four seven nine five seven six\n",
      "189373### \t one six one eight four three three nine\n",
      "778733355 \t five seven one nine eight seven\n",
      "339977777 \t seven eight seven three one three five five\n",
      "833959999 \t nine two seven eight five six two four\n",
      "576555### \t five four one four five\n",
      "141517633 \t five six two nine three eight two\n",
      "281773### \t three four nine three one\n",
      "4334449## \t six six six one one three\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attn = Attention(NUM_UNITS_ENC)\n",
    "encoder = EncoderRNN(NUM_INPUTS, HIDDEN_DIM, NUM_UNITS_ENC, DROPOUT).to(device)\n",
    "decoder = DecoderRNN(NUM_UNITS_DEC, HIDDEN_DIM, NUM_OUTPUTS, DROPOUT, attn).to(\n",
    "    device\n",
    ")\n",
    "enc_optimizer = optim.RMSprop(encoder.parameters(), lr=LEARNING_RATE)\n",
    "dec_optimizer = optim.RMSprop(decoder.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=0)\n",
    "\n",
    "# Get training set\n",
    "\n",
    "inputs,_,targets_in,targets,targets_seqlen,_,text,_,text_targ,inputs_len= generate(\n",
    "    TRAINING_SIZE, NUM_OF_BATCHES, min_len=MIN_SEQ_LEN, max_len=MAX_SEQ_LEN\n",
    ")\n",
    "max_target_len = max(targets_seqlen)\n",
    "unique_text_targets = set([i for x in text_targ for i in x])\n",
    "\n",
    "# Get validation set\n",
    "(\n",
    "    val_inputs,_,val_targets_in,val_targets,val_targets_seqlen,_,\n",
    "    val_text_in,_,val_text_targ,val_inputs_len\n",
    ") = generate(\n",
    "    1,\n",
    "    TEST_SIZE,\n",
    "    min_len=MIN_SEQ_LEN,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    invalid_set=unique_text_targets,\n",
    ")\n",
    "\n",
    "val_inputs = torch.LongTensor(val_inputs).to(device)\n",
    "val_targets = torch.LongTensor(val_targets).to(device)\n",
    "val_targets_in = torch.LongTensor(val_targets_in).to(device)\n",
    "val_inputs_len = torch.LongTensor(val_inputs_len).to(device)\n",
    "max_val_target_len = max(val_targets_seqlen)\n",
    "\n",
    "\n",
    "# Quick and dirty - just loop over training set without reshuffling\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        inputs,\n",
    "        targets,\n",
    "        targets_in,\n",
    "        criterion,\n",
    "        enc_optimizer,\n",
    "        dec_optimizer,\n",
    "        epoch,\n",
    "        inputs_len\n",
    "    )\n",
    "    _, loss, accuracy = test(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        val_inputs,\n",
    "        val_targets,\n",
    "        val_targets_in,\n",
    "        val_inputs_len,\n",
    "        criterion\n",
    "    )\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f} \\tAccuracy: {:.3f}%\\n\".format(\n",
    "            loss, accuracy.item() * 100.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show examples\n",
    "    print(\"Examples: prediction | input\")\n",
    "    out, _, _ = test(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        val_inputs[:10],\n",
    "        val_targets[:10],\n",
    "        val_targets_in[:10],\n",
    "        val_inputs_len[:10],\n",
    "        criterion\n",
    "    )\n",
    "    pred = get_pred(out)\n",
    "    pred_text = [numbers_to_text(sample) for sample in pred]\n",
    "    for i in range(9):\n",
    "        print(pred_text[i], \"\\t\", val_text_in[0][i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5.4-EXE-seq2seq-digits_Amalia.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
